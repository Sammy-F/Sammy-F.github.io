{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"weo2I190Qrb9","colab_type":"text"},"cell_type":"markdown","source":["# Comp 440 HW3: Mining Sentiment on Twitter\n","## Samantha Fritsche\n","### 3/26/2020\n"]},{"metadata":{"id":"Vn6SAjZqQqSV","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vbsOlVoRecKG","colab_type":"text"},"cell_type":"markdown","source":["### Data Collection"]},{"metadata":{"id":"vjXbuxwieeVp","colab_type":"text"},"cell_type":"markdown","source":["I collected and cleaned my data on my local machine, then uploaded the json files to Dropbox. While collecting my labeled tweet dataset, I confirmed that I given tweet contained only \":)\" or \":(,\" but not both. In addition, I stripped out usernames, links, punctuation, and stopwords. I also skipped retweets so that any single tweet by a popular individual would not be given greater weight than any other tweet. I preserved the original text in the \"text\" field and stored the cleaned text in the \"clean_text\" field. In addition, I assigned tweets a sentiment based on whether they included \":),\" \":(,\" or neither - \"happy,\" \"sad,\" or \"?,\" respectively. The last sentiment category only came into play as I was collecting my keyword-based dataset. I chose to include tweets containing the keyword, \"healthcare.\" I did not deliberately collect tweets with either emoticon in them, but I did allow them. I've included below a code sample showing how I gathered my \"happy\" dataset. For all of the datasets, I avoided duplicating tweets by ensuring that, for each new stream tweet, its ID did not match any of the past 100 ID's.\n","\n","Overall, I collected 32,500 tweets - 15k \"happy\" tweets, 15k \"sad\" tweets, and 2.5k \"healthcare\" tweets."]},{"metadata":{"id":"RQck_vxwgY94","colab_type":"text"},"cell_type":"markdown","source":["import tweepy\n","\n","import json, string\n","\n","import re\n","\n","from nltk.corpus import stopwords\n","\n","\n","class MyStreamListener(tweepy.StreamListener):\n","\n","    def __init__(self, stop_at=15000):\n","        super(MyStreamListener, self).__init__()\n","        self.tweet_counter = 0\n","        self.datalist = []\n","        self.stop_at = stop_at\n","        self.file_entries = 0\n","        self.checkpoint = 100\n","\n","        self.file_cap = 5000\n","\n","        self.ids = [0] * 100\n","        self.id_exists = False\n","        self.stops = stopwords.words(\"english\")\n","\n","\n","    def on_status(self, status):\n","\n","        # Write a file every 5000 entries read\n","        # If the connection messes up, we'll have some of the stream at least\n","\n","        if self.tweet_counter % self.checkpoint == 0:\n","            print(self.tweet_counter)\n","\n","        if len(self.datalist) >= self.file_cap:\n","            print('file created')\n","            with open('nsmile_tweets_' + str(self.file_entries) + '.json', 'w') as outfile:\n","                json.dump(self.datalist, outfile)\n","            self.file_entries += 1\n","            self.datalist.clear()\n","\n","        if self.tweet_counter < self.stop_at:\n","            tweetj = status._json\n","\n","            if tweetj['id'] not in self.ids:\n","                self.id_exists = False\n","            else:\n","                self.id_exists = True\n","\n","            if 'retweeted_status' not in tweetj and not self.id_exists:\n","                if (tweetj['lang']=='en'):\n","\n","\n","                    text = ''\n","                    if 'extended_tweet' in tweetj:\n","                        # print('hit full')\n","                        text = tweetj['extended_tweet']['full_text'] # Make sure to get full tweet text\n","                    else:\n","                        text = tweetj['text']\n","\n","                    tweetj['text'] = text\n","\n","                    # ----------- Set invalids -----------\n","                    if ':)' in text:  # skip messages without certain things\n","\n","                        if ':(' not in text:\n","\n","                            # print(\"Orig: \" + tweetj['text'])\n","\n","                            text = text.lower() # Make everything lower\n","                            text = re.sub('http://\\S+|https://\\S+', '', text)  # Remove links\n","                            text = re.sub('@[^\\s]+', '', text)  # Remove usernames\n","                            text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n","                            text = ' '.join([word for word in text.split() if word not in (self.stops)]) # Remove stopwords\n","                            # print(\"New: \" + text)\n","\n","                            tweetj['clean_text'] = text\n","\n","                            # ----------- Set sentiment start -----------\n","                            tweetj['sentiment'] = 'happy'\n","                            # ----------- Set sentiment end -----------\n","\n","                            self.datalist.append(tweetj)\n","\n","                            self.tweet_counter += 1\n","                            self.ids.pop()\n","                            self.ids.append(tweetj['id'])\n","            return True\n","\n","        else:\n","            return False\n","\n","consumer_token = 'xxx'\n","consumer_secret = 'xxx'\n","\n","access_token = 'xxx'\n","access_secret = 'xxx'\n","\n","auth = tweepy.OAuthHandler(consumer_token, consumer_secret)\n","\n","auth.set_access_token(access_token, access_secret)\n","\n","listener = MyStreamListener(stop_at=15000)\n","\n","streaming_api = tweepy.Stream(auth, listener, timeout=60, tweet_mode='extended')\n","\n","streaming_api.filter(track=[':)'])\n"]},{"metadata":{"id":"n5npvGwMhO-H","colab_type":"text"},"cell_type":"markdown","source":["### Data Preparation"]},{"metadata":{"id":"qbOBY2dXhVsQ","colab_type":"text"},"cell_type":"markdown","source":["To prepare my data for use as a training data set, I concatenated all of the data I'd collected into a single pandas DataFrame. I only included my text and sentiment data as I was only interested in doing text analysis."]},{"metadata":{"id":"Dq2awaYUZx2p","colab_type":"code","colab":{}},"cell_type":"code","source":["sm1_df = pd.read_json('https://www.dropbox.com/s/gp1nuytnu42mbm1/nsmile_tweets_0.json?dl=1')[['clean_text', 'text', 'sentiment']]\n","sm2_df = pd.read_json('https://www.dropbox.com/s/1qh2hxgpzpz530q/nsmile_tweets_1.json?dl=1')[['clean_text', 'text', 'sentiment']]\n","sm3_df = pd.read_json('https://www.dropbox.com/s/dnnbi6p7kswqjqj/nsmile_tweets_2.json?dl=1')[['clean_text', 'text', 'sentiment']]\n","\n","fr1_df = pd.read_json('https://www.dropbox.com/s/donahmoq4uhybme/nfrown_tweets_0.json?dl=1')[['clean_text', 'text', 'sentiment']]\n","fr2_df = pd.read_json('https://www.dropbox.com/s/e885oryqk4q3czx/nfrown_tweets_1.json?dl=1')[['clean_text', 'text', 'sentiment']]\n","fr3_df = pd.read_json('https://www.dropbox.com/s/0oycgnwbao086d4/nfrown_tweets_2.json?dl=1')[['clean_text', 'text', 'sentiment']]\n","\n","misc_sm_df = pd.read_json('https://www.dropbox.com/s/apxxw40v3jo3532/nmisc_smile_tweets.json?dl=1')[['clean_text', 'text', 'sentiment']]\n","misc_fr_df = pd.read_json('https://www.dropbox.com/s/uqsw3zf2e3y1tl3/nmisc_frown_tweets.json?dl=1')[['clean_text', 'text', 'sentiment']]\n","misc_qu_df = pd.read_json('https://www.dropbox.com/s/s6pz9q94uqhcd1m/nmisc_quest_tweets.json?dl=1')[['clean_text', 'text', 'sentiment']]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kWX2E0LWcSZQ","colab_type":"code","outputId":"c9a699a7-28d3-4ab5-e22f-4a0d08158c11","executionInfo":{"status":"ok","timestamp":1553571204587,"user_tz":300,"elapsed":35897,"user":{"displayName":"Samantha Fritsche","photoUrl":"","userId":"12778135598875463896"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"cell_type":"code","source":["tweet_df = pd.concat([sm1_df, sm2_df, sm3_df, fr1_df, fr2_df, fr3_df, misc_sm_df, misc_fr_df, misc_qu_df], ignore_index=True).reset_index(drop=True)\n","tweet_df.head()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>passport renewal took like hour half bad</td>\n","      <td>Passport renewal only took like an hour and a half, not bad :)</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>oh goodness good one</td>\n","      <td>@mpatte10 Oh my goodness! Good one! :)</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>say jumped plane fell rainbownowell read happened skydiving skydivingnewengland maine</td>\n","      <td>Can you say you have jumped out of a plane, and then fell through a rainbow?...No?......Well I can, because I did  :) \\n.\\nRead how it happened here https://t.co/iYekJ3g5om\\n.\\n#Skydiving #SkydivingNewEngland #Maine https://t.co/L9iiKYGLDw</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>course</td>\n","      <td>@gloriagaynor But, of course! :)</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>oh didnt know</td>\n","      <td>@JonComms Oh my. Didn't know this :)</td>\n","      <td>happy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                              clean_text  \\\n","0  passport renewal took like hour half bad                                                \n","1  oh goodness good one                                                                    \n","2  say jumped plane fell rainbownowell read happened skydiving skydivingnewengland maine   \n","3  course                                                                                  \n","4  oh didnt know                                                                           \n","\n","                                                                                                                                                                                                                                              text  \\\n","0  Passport renewal only took like an hour and a half, not bad :)                                                                                                                                                                                    \n","1  @mpatte10 Oh my goodness! Good one! :)                                                                                                                                                                                                            \n","2  Can you say you have jumped out of a plane, and then fell through a rainbow?...No?......Well I can, because I did  :) \\n.\\nRead how it happened here https://t.co/iYekJ3g5om\\n.\\n#Skydiving #SkydivingNewEngland #Maine https://t.co/L9iiKYGLDw   \n","3  @gloriagaynor But, of course! :)                                                                                                                                                                                                                  \n","4  @JonComms Oh my. Didn't know this :)                                                                                                                                                                                                              \n","\n","  sentiment  \n","0  happy     \n","1  happy     \n","2  happy     \n","3  happy     \n","4  happy     "]},"metadata":{"tags":[]},"execution_count":32}]},{"metadata":{"id":"5_pANzXUdLJB","colab_type":"text"},"cell_type":"markdown","source":["After concatenating the data, I constructed a Tf-Idf matrix for all of the data."]},{"metadata":{"id":"s4Ki-QmPe8kU","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","\n","count_vect = CountVectorizer()\n","counts = count_vect.fit_transform(tweet_df['clean_text'])\n","\n","tfidf_transformer = TfidfTransformer()\n","full_tfidf = tfidf_transformer.fit_transform(counts)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_uWudr9Odj3M","colab_type":"code","outputId":"bb055fa6-304d-4284-cb75-920975841503","executionInfo":{"status":"ok","timestamp":1553571205156,"user_tz":300,"elapsed":36436,"user":{"displayName":"Samantha Fritsche","photoUrl":"","userId":"12778135598875463896"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["display(full_tfidf)"],"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":["<32500x29578 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 240337 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"MM62PZHIdRCg","colab_type":"text"},"cell_type":"markdown","source":["Since I reset the indices of my DataFrame, the array row indices coordinate with the index of my DataFrame. Each row of the matrix represents a tweet, with each of the columns representing a one-word \"feature\" that tweet includes. Below I printed information about the first three rows of the matrix, AKA the first three tweets in the DataFrame, which can be seen above. If you compare the \"clean_text\" field to the number of stored elements, you can see the number of words in the former is equal to the latter."]},{"metadata":{"id":"yyRpfDgSkHPZ","colab_type":"code","outputId":"50d8d2cf-518d-48da-fc32-b789fb299742","executionInfo":{"status":"ok","timestamp":1553571205158,"user_tz":300,"elapsed":36419,"user":{"displayName":"Samantha Fritsche","photoUrl":"","userId":"12778135598875463896"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"cell_type":"code","source":["display(full_tfidf[0,:])\n","display(full_tfidf[1,:])\n","display(full_tfidf[2,:])"],"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":["<1x29578 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 7 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<1x29578 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 4 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["<1x29578 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 10 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"kZsC8VlXlKSc","colab_type":"text"},"cell_type":"markdown","source":["However, although I want my Tf-Idf matrix to include features for all of the data, I want to train my classifier only based on the labeled dataset and test the classifier on unlabeled data. Therefore, I needed to break the matrix up into two subsets - labeled and unlabeled. I then broke the labeled subset up into test and training data."]},{"metadata":{"id":"4mkVdFQDDFnO","colab_type":"code","colab":{}},"cell_type":"code","source":["tweet_df_labeled = tweet_df[tweet_df['sentiment']!='?']\n","\n","tweet_df_unlabeled = tweet_df[tweet_df['sentiment']=='?'].copy() # denote rows to remove\n","mask_indices = list(tweet_df_unlabeled.index) # array position indices"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OF76U4sdE9ws","colab_type":"code","colab":{}},"cell_type":"code","source":["mask = np.ones(full_tfidf.shape[0], dtype=bool)\n","mask[mask_indices] = False\n","\n","labeled_tfidf = full_tfidf[mask]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m1USrF8HdfzI","colab_type":"code","outputId":"badabd15-c310-4354-d1ba-07e9509fd569","executionInfo":{"status":"ok","timestamp":1553571205166,"user_tz":300,"elapsed":36381,"user":{"displayName":"Samantha Fritsche","photoUrl":"","userId":"12778135598875463896"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["labeled_tfidf"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<30003x29578 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 204868 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":38}]},{"metadata":{"id":"Jx41aBB0a7Bh","colab_type":"code","colab":{}},"cell_type":"code","source":["(labeled_tfidf_train, labeled_tfidf_test, labeled_train, labeled_test) = train_test_split(labeled_tfidf, \n","                                                                  tweet_df_labeled['sentiment'], \n","                                                                  test_size=0.2, random_state=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DtsoyfIHcwku","colab_type":"text"},"cell_type":"markdown","source":["Using the training data, I fit a Gradient Boosting Classifier. I also produced the confusion matrix for the test data. My classifier achieved an accuracy of approximately 68%. This is a relatively low score, therefore I later attempted to improve it by including bigrams as feature, in the next section."]},{"metadata":{"id":"m7TyRh3RhlJq","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import confusion_matrix\n","\n","gbc = GradientBoostingClassifier().fit(labeled_tfidf_train, labeled_train)\n","labeled_pred = gbc.predict(labeled_tfidf_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"shC9DzlrjcSR","colab_type":"code","outputId":"547ff7ef-e97b-4be7-ab9e-eedaaaf496d6","executionInfo":{"status":"ok","timestamp":1553571253068,"user_tz":300,"elapsed":84242,"user":{"displayName":"Samantha Fritsche","photoUrl":"","userId":"12778135598875463896"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["c_matrix = confusion_matrix(labeled_test, labeled_pred)\n","display(c_matrix)\n","display((c_matrix[0,0]+c_matrix[1,1])/(c_matrix[1,0]+c_matrix[0,1]+c_matrix[0,0]+c_matrix[1,1]))"],"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":["array([[2678,  346],\n","       [1554, 1423]])"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["0.6833861023162806"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"7JWC-GKyAzJT","colab_type":"text"},"cell_type":"markdown","source":["I was also interested in seeing the most \"important\" features in determining sentiment. Logically, words like \"miss,\" \"sad,\" and \"thanks,\" which generally have a clear positive/negative skew, are high on  the list."]},{"metadata":{"id":"5w5s9w9QgYWE","colab_type":"code","outputId":"79020f72-b6f3-4b9f-90db-eb0230ae97a5","executionInfo":{"status":"ok","timestamp":1553571253070,"user_tz":300,"elapsed":84229,"user":{"displayName":"Samantha Fritsche","photoUrl":"","userId":"12778135598875463896"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"cell_type":"code","source":["top_indices = gbc.feature_importances_.argsort()[::-1]\n","names = count_vect.get_feature_names()\n","print([names[i] for i in top_indices[:30]])"],"execution_count":42,"outputs":[{"output_type":"stream","text":["['miss', 'sad', 'thanks', 'sorry', 'want', 'thank', 'love', 'great', 'wish', 'good', 'wanna', 'baby', 'heart', 'happy', 'omg', 'new', 'nice', 'pls', 'hi', 'supernatural', 'welcome', 'cute', 'much', 'check', 'awesome', 'enjoy', 'poor', 'bad', 'hey', 'im']\n"],"name":"stdout"}]},{"metadata":{"id":"kKYSqMdTc25k","colab_type":"text"},"cell_type":"markdown","source":["Next, I examined predictions for the unlabeled dataset. To do this, I subset the Tf-Idf matrix again, but this time produced only the unlabeled rows. In addition to the hard \"happy\" and \"sad\" predictions, I produced a probability or \"score\" of happiness/sadness for each tweet. I appended these to my tweet DataFrame."]},{"metadata":{"id":"hIyS3S1QfEc9","colab_type":"code","colab":{}},"cell_type":"code","source":["n_mask_indices = list(tweet_df_labeled.index) # Recall that these are rows that are \"happy\" or \"sad\"\n","n_mask = np.ones(full_tfidf.shape[0], dtype=bool)\n","n_mask[n_mask_indices] = False\n","\n","unlabeled_tfidf = full_tfidf[n_mask]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lv2LuTMBftut","colab_type":"code","colab":{}},"cell_type":"code","source":["unlabeled_pred = list(gbc.predict(unlabeled_tfidf))\n","unlabeled_pred_probs = gbc.predict_proba(unlabeled_tfidf)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O8e4YKKfn5lv","colab_type":"code","colab":{}},"cell_type":"code","source":["happy_probs = unlabeled_pred_probs[:,0]\n","sad_probs = unlabeled_pred_probs[:,1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xv4DbgY5f16N","colab_type":"code","outputId":"34ed2851-e78a-49c1-a862-002bb444990b","executionInfo":{"status":"ok","timestamp":1553571253221,"user_tz":300,"elapsed":84298,"user":{"displayName":"Samantha Fritsche","photoUrl":"","userId":"12778135598875463896"}},"colab":{"base_uri":"https://localhost:8080/","height":500}},"cell_type":"code","source":["pd.set_option('display.max_colwidth', -1)\n","\n","tweet_df_unlabeled['pred'] = unlabeled_pred\n","\n","tweet_df_unlabeled['happiness'] = happy_probs\n","tweet_df_unlabeled['sadness'] = sad_probs\n","\n","display(tweet_df_unlabeled.sort_values(by='happiness', ascending=False).head())\n","display(tweet_df_unlabeled.sort_values(by='sadness', ascending=False).head())"],"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>pred</th>\n","      <th>happiness</th>\n","      <th>sadness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>31342</th>\n","      <td>latest chronically awesome reader thanks healthcare parkinsons</td>\n","      <td>The latest The Chronically Awesome Reader! https://t.co/CcU7YhoAQA Thanks to @IsobelKnight2 #healthcare #parkinsons</td>\n","      <td>?</td>\n","      <td>happy</td>\n","      <td>0.898682</td>\n","      <td>0.101318</td>\n","    </tr>\n","    <tr>\n","      <th>32037</th>\n","      <td>hi andrew thanks support working parent thrilled taxes support affordable childcare well healthcare ways society cares seniors</td>\n","      <td>@oceansidewebtv @albertaNDP @RachelNotley Hi Andrew, thanks for your support. As a working parent, I am thrilled for my taxes support more affordable childcare - as well as healthcare and other ways our society cares for seniors.</td>\n","      <td>?</td>\n","      <td>happy</td>\n","      <td>0.887219</td>\n","      <td>0.112781</td>\n","    </tr>\n","    <tr>\n","      <th>31983</th>\n","      <td>thanks she’s going fine minor injury thankfully super glad live rad country public healthcare</td>\n","      <td>@ScottCrockatt Thanks! She’s going to be fine, just a minor injury thankfully. Super glad we live in such a rad country with public healthcare!</td>\n","      <td>?</td>\n","      <td>happy</td>\n","      <td>0.883968</td>\n","      <td>0.116032</td>\n","    </tr>\n","    <tr>\n","      <th>31761</th>\n","      <td>thanks jeff looking forward</td>\n","      <td>Thanks Jeff. Looking forward to it.   https://t.co/6CJdmgzcgL</td>\n","      <td>?</td>\n","      <td>happy</td>\n","      <td>0.873127</td>\n","      <td>0.126873</td>\n","    </tr>\n","    <tr>\n","      <th>32477</th>\n","      <td>someone preexisting condition needs health insurance survive id rather healthcare fucked good guys win political points thanks</td>\n","      <td>@MmeScience @sahilkapur As someone with a pre-existing condition who needs health insurance to survive I'd rather not have my healthcare fucked up so \"the good guys\" can win political points, thanks</td>\n","      <td>?</td>\n","      <td>happy</td>\n","      <td>0.843994</td>\n","      <td>0.156006</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                           clean_text  \\\n","31342  latest chronically awesome reader thanks healthcare parkinsons                                                                   \n","32037  hi andrew thanks support working parent thrilled taxes support affordable childcare well healthcare ways society cares seniors   \n","31983  thanks she’s going fine minor injury thankfully super glad live rad country public healthcare                                    \n","31761  thanks jeff looking forward                                                                                                      \n","32477  someone preexisting condition needs health insurance survive id rather healthcare fucked good guys win political points thanks   \n","\n","                                                                                                                                                                                                                                        text  \\\n","31342  The latest The Chronically Awesome Reader! https://t.co/CcU7YhoAQA Thanks to @IsobelKnight2 #healthcare #parkinsons                                                                                                                     \n","32037  @oceansidewebtv @albertaNDP @RachelNotley Hi Andrew, thanks for your support. As a working parent, I am thrilled for my taxes support more affordable childcare - as well as healthcare and other ways our society cares for seniors.   \n","31983  @ScottCrockatt Thanks! She’s going to be fine, just a minor injury thankfully. Super glad we live in such a rad country with public healthcare!                                                                                         \n","31761  Thanks Jeff. Looking forward to it.   https://t.co/6CJdmgzcgL                                                                                                                                                                           \n","32477  @MmeScience @sahilkapur As someone with a pre-existing condition who needs health insurance to survive I'd rather not have my healthcare fucked up so \"the good guys\" can win political points, thanks                                  \n","\n","      sentiment   pred  happiness   sadness  \n","31342  ?         happy  0.898682   0.101318  \n","32037  ?         happy  0.887219   0.112781  \n","31983  ?         happy  0.883968   0.116032  \n","31761  ?         happy  0.873127   0.126873  \n","32477  ?         happy  0.843994   0.156006  "]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>pred</th>\n","      <th>happiness</th>\n","      <th>sadness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>31682</th>\n","      <td>sad thing education healthcare jobs poor sods want jumlaman blogman mercenaries think otherwise</td>\n","      <td>@tavleen_singh And the sad thing is education healthcare jobs is what the poor sods want, but JumlaMan, BlogMan and their mercenaries think otherwise. https://t.co/vls1v4vMLv</td>\n","      <td>?</td>\n","      <td>sad</td>\n","      <td>0.091702</td>\n","      <td>0.908298</td>\n","    </tr>\n","    <tr>\n","      <th>31057</th>\n","      <td>sad hear jeff work healthcare amp breaks heart desperately need medicare</td>\n","      <td>@JeffHisDudeness So sad to hear this Jeff. I work in healthcare &amp;amp; this just breaks my heart. We desperately need Medicare for ALL.</td>\n","      <td>?</td>\n","      <td>sad</td>\n","      <td>0.092471</td>\n","      <td>0.907529</td>\n","    </tr>\n","    <tr>\n","      <th>30307</th>\n","      <td>miserable human must someone want access healthcare ripped away sick people sad</td>\n","      <td>@RickLangel @thehill How miserable of a human being must someone be to want access to healthcare to be ripped away from sick people?  Sad.</td>\n","      <td>?</td>\n","      <td>sad</td>\n","      <td>0.135246</td>\n","      <td>0.864754</td>\n","    </tr>\n","    <tr>\n","      <th>31075</th>\n","      <td>miss vote healthcare bill</td>\n","      <td>@dccc Did I miss the vote on @RepJayapal's healthcare bill?</td>\n","      <td>?</td>\n","      <td>sad</td>\n","      <td>0.160904</td>\n","      <td>0.839096</td>\n","    </tr>\n","    <tr>\n","      <th>30509</th>\n","      <td>american healthcare system damn messed neglected it’s sad</td>\n","      <td>the American healthcare system is so damn messed up and neglected it’s sad</td>\n","      <td>?</td>\n","      <td>sad</td>\n","      <td>0.164510</td>\n","      <td>0.835490</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                            clean_text  \\\n","31682  sad thing education healthcare jobs poor sods want jumlaman blogman mercenaries think otherwise   \n","31057  sad hear jeff work healthcare amp breaks heart desperately need medicare                          \n","30307  miserable human must someone want access healthcare ripped away sick people sad                   \n","31075  miss vote healthcare bill                                                                         \n","30509  american healthcare system damn messed neglected it’s sad                                         \n","\n","                                                                                                                                                                                 text  \\\n","31682  @tavleen_singh And the sad thing is education healthcare jobs is what the poor sods want, but JumlaMan, BlogMan and their mercenaries think otherwise. https://t.co/vls1v4vMLv   \n","31057  @JeffHisDudeness So sad to hear this Jeff. I work in healthcare &amp; this just breaks my heart. We desperately need Medicare for ALL.                                           \n","30307  @RickLangel @thehill How miserable of a human being must someone be to want access to healthcare to be ripped away from sick people?  Sad.                                       \n","31075  @dccc Did I miss the vote on @RepJayapal's healthcare bill?                                                                                                                      \n","30509  the American healthcare system is so damn messed up and neglected it’s sad                                                                                                       \n","\n","      sentiment pred  happiness   sadness  \n","31682  ?         sad  0.091702   0.908298  \n","31057  ?         sad  0.092471   0.907529  \n","30307  ?         sad  0.135246   0.864754  \n","31075  ?         sad  0.160904   0.839096  \n","30509  ?         sad  0.164510   0.835490  "]},"metadata":{"tags":[]}}]},{"metadata":{"id":"EAiL10bSB957","colab_type":"text"},"cell_type":"markdown","source":["Although my model doesn't perform very well in general, it does appear that it has correctly predicted the sentiment for the happiest and saddest of posts for the most part. Tweet #32477 is clearly negative. It's likely that certain words like \"thanks\" and \"good,\" which are generally positive, caused it to be skewed positive."]},{"metadata":{"id":"IaP3vALBkuCe","colab_type":"text"},"cell_type":"markdown","source":["### Bigrams"]},{"metadata":{"id":"bO47bLvvT6Q1","colab_type":"text"},"cell_type":"markdown","source":["My last classifier uses only unigrams as features and is obviously not terribly accurate. In order to improve the classifier, I introduced bigrams as well. This allows us to consider phrases such as \"not good\" as being collectively negative, even if \"not\" and \"good\" might not be negative on their own. The workflow was largely the same as for the unigram classifier. I produced a Tf-Idf matrix for the data, but this time included both unigrams and bigrams as features. I then subset the labeled dataset and trained the model on it, then tested the model on the test labeled data and the unlabeled data."]},{"metadata":{"id":"rnH-7DAVktTm","colab_type":"code","colab":{}},"cell_type":"code","source":["bi_count_vect = CountVectorizer(ngram_range=(1,2))\n","bi_counts = bi_count_vect.fit_transform(tweet_df['clean_text'])\n","\n","bi_tfidf_transformer = TfidfTransformer()\n","bi_full_tfidf = bi_tfidf_transformer.fit_transform(bi_counts)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tYDEaYmgkiDX","colab_type":"code","colab":{}},"cell_type":"code","source":["bi_mask = np.ones(bi_full_tfidf.shape[0], dtype=bool)\n","bi_mask[mask_indices] = False\n","\n","bi_labeled_tfidf_cl = bi_full_tfidf[bi_mask]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qgzEeap-ke2c","colab_type":"code","colab":{}},"cell_type":"code","source":["(bi_labeled_tfidf_train, bi_labeled_tfidf_test, bi_labeled_train, bi_labeled_test) = train_test_split(bi_labeled_tfidf_cl, \n","                                                                  tweet_df_labeled['sentiment'], \n","                                                                  test_size=0.2, random_state=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oxKR-GpYk0cY","colab_type":"code","colab":{}},"cell_type":"code","source":["bi_gbc = GradientBoostingClassifier().fit(bi_labeled_tfidf_train, bi_labeled_train)\n","bi_labeled_pred = bi_gbc.predict(bi_labeled_tfidf_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n2v4znV5DMt7","colab_type":"text"},"cell_type":"markdown","source":["I show the confusion matrix for this model below. I was disappointed to discover that the model's accuracy was nearly the same.. Moreover, when I displayed the most important features, I discovered that they were all unigram features. This could potentially be a negative effect of having removed stopwords at the start. With further work, I would consider leaving some or all stopwords in."]},{"metadata":{"id":"9KN4cwl0lnyd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"e8209ff4-af58-440b-f565-3360b188aaff","executionInfo":{"status":"ok","timestamp":1553571608059,"user_tz":300,"elapsed":439071,"user":{"displayName":"Samantha Fritsche","photoUrl":"","userId":"12778135598875463896"}}},"cell_type":"code","source":["bi_c_matrix = confusion_matrix(bi_labeled_test, bi_labeled_pred)\n","display(bi_c_matrix)\n","display((bi_c_matrix[0,0]+bi_c_matrix[1,1])/(bi_c_matrix[0,0]+bi_c_matrix[1,1]+bi_c_matrix[0,1]+bi_c_matrix[1,0]))"],"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":["array([[2623,  401],\n","       [1507, 1470]])"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["0.6820529911681387"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"72oft6Pfk30T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"98cce7b8-2b88-4c1c-fd99-d2d3cc3e7298","executionInfo":{"status":"ok","timestamp":1553571608682,"user_tz":300,"elapsed":439683,"user":{"displayName":"Samantha Fritsche","photoUrl":"","userId":"12778135598875463896"}}},"cell_type":"code","source":["bi_top_indexes = bi_gbc.feature_importances_.argsort()[::-1]\n","bi_names = bi_count_vect.get_feature_names()\n","print([bi_names[i] for i in bi_top_indexes[:30]])"],"execution_count":52,"outputs":[{"output_type":"stream","text":["['miss', 'sad', 'thanks', 'sorry', 'want', 'thank', 'love', 'wish', 'great', 'good', 'wanna', 'baby', 'happy', 'heart', 'omg', 'new', 'nice', 'hi', 'pls', 'supernatural', 'cute', 'welcome', 'awesome', 'check', 'much', 'oh', 'im', 'enjoy', 'poor', 'hey']\n"],"name":"stdout"}]},{"metadata":{"id":"OLO9R40elmzR","colab_type":"code","colab":{}},"cell_type":"code","source":["bi_unlabeled_mask = np.ones(bi_full_tfidf.shape[0], dtype=bool)\n","bi_unlabeled_mask[n_mask_indices] = False\n","\n","bi_unlabeled_tfidf = bi_full_tfidf[bi_unlabeled_mask]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EjvF6_AaouNp","colab_type":"code","colab":{}},"cell_type":"code","source":["bi_unlabeled_pred = list(bi_gbc.predict(bi_unlabeled_tfidf))\n","bi_unlabeled_pred_probs = bi_gbc.predict_proba(bi_unlabeled_tfidf)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s09WkdtUoxYk","colab_type":"code","colab":{}},"cell_type":"code","source":["bi_happy_probs = bi_unlabeled_pred_probs[:,0]\n","bi_sad_probs = bi_unlabeled_pred_probs[:,1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LzYYLunOozkx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":535},"outputId":"5c29871c-ec8e-4453-c7bd-6b5bba969a6a","executionInfo":{"status":"ok","timestamp":1553571836306,"user_tz":300,"elapsed":360,"user":{"displayName":"Samantha Fritsche","photoUrl":"","userId":"12778135598875463896"}}},"cell_type":"code","source":["pd.set_option('display.max_colwidth', -1)\n","\n","tweet_df_unlabeled['bi_pred'] = bi_unlabeled_pred\n","\n","tweet_df_unlabeled['bi_happiness'] = bi_happy_probs\n","tweet_df_unlabeled['bi_sadness'] = bi_sad_probs\n","\n","display(tweet_df_unlabeled.sort_values(by='bi_happiness', ascending=False).head(5))\n","display(tweet_df_unlabeled.sort_values(by='bi_sadness', ascending=False).head(5))"],"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>pred</th>\n","      <th>happiness</th>\n","      <th>sadness</th>\n","      <th>bi_pred</th>\n","      <th>bi_happiness</th>\n","      <th>bi_sadness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>31342</th>\n","      <td>latest chronically awesome reader thanks healthcare parkinsons</td>\n","      <td>The latest The Chronically Awesome Reader! https://t.co/CcU7YhoAQA Thanks to @IsobelKnight2 #healthcare #parkinsons</td>\n","      <td>?</td>\n","      <td>happy</td>\n","      <td>0.898682</td>\n","      <td>0.101318</td>\n","      <td>happy</td>\n","      <td>0.899134</td>\n","      <td>0.100866</td>\n","    </tr>\n","    <tr>\n","      <th>32037</th>\n","      <td>hi andrew thanks support working parent thrilled taxes support affordable childcare well healthcare ways society cares seniors</td>\n","      <td>@oceansidewebtv @albertaNDP @RachelNotley Hi Andrew, thanks for your support. As a working parent, I am thrilled for my taxes support more affordable childcare - as well as healthcare and other ways our society cares for seniors.</td>\n","      <td>?</td>\n","      <td>happy</td>\n","      <td>0.887219</td>\n","      <td>0.112781</td>\n","      <td>happy</td>\n","      <td>0.886858</td>\n","      <td>0.113142</td>\n","    </tr>\n","    <tr>\n","      <th>31983</th>\n","      <td>thanks she’s going fine minor injury thankfully super glad live rad country public healthcare</td>\n","      <td>@ScottCrockatt Thanks! She’s going to be fine, just a minor injury thankfully. Super glad we live in such a rad country with public healthcare!</td>\n","      <td>?</td>\n","      <td>happy</td>\n","      <td>0.883968</td>\n","      <td>0.116032</td>\n","      <td>happy</td>\n","      <td>0.877798</td>\n","      <td>0.122202</td>\n","    </tr>\n","    <tr>\n","      <th>31761</th>\n","      <td>thanks jeff looking forward</td>\n","      <td>Thanks Jeff. Looking forward to it.   https://t.co/6CJdmgzcgL</td>\n","      <td>?</td>\n","      <td>happy</td>\n","      <td>0.873127</td>\n","      <td>0.126873</td>\n","      <td>happy</td>\n","      <td>0.870848</td>\n","      <td>0.129152</td>\n","    </tr>\n","    <tr>\n","      <th>30498</th>\n","      <td>follow laws country congress work together come better healthcare mind ur country thanks</td>\n","      <td>@SatchofBridgend @PittMom16 @thehill It is. But we follow laws in this country. Congress can work together to come up with better healthcare. Until mind ur own country thanks</td>\n","      <td>?</td>\n","      <td>happy</td>\n","      <td>0.831058</td>\n","      <td>0.168942</td>\n","      <td>happy</td>\n","      <td>0.842282</td>\n","      <td>0.157718</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                           clean_text  \\\n","31342  latest chronically awesome reader thanks healthcare parkinsons                                                                   \n","32037  hi andrew thanks support working parent thrilled taxes support affordable childcare well healthcare ways society cares seniors   \n","31983  thanks she’s going fine minor injury thankfully super glad live rad country public healthcare                                    \n","31761  thanks jeff looking forward                                                                                                      \n","30498  follow laws country congress work together come better healthcare mind ur country thanks                                         \n","\n","                                                                                                                                                                                                                                        text  \\\n","31342  The latest The Chronically Awesome Reader! https://t.co/CcU7YhoAQA Thanks to @IsobelKnight2 #healthcare #parkinsons                                                                                                                     \n","32037  @oceansidewebtv @albertaNDP @RachelNotley Hi Andrew, thanks for your support. As a working parent, I am thrilled for my taxes support more affordable childcare - as well as healthcare and other ways our society cares for seniors.   \n","31983  @ScottCrockatt Thanks! She’s going to be fine, just a minor injury thankfully. Super glad we live in such a rad country with public healthcare!                                                                                         \n","31761  Thanks Jeff. Looking forward to it.   https://t.co/6CJdmgzcgL                                                                                                                                                                           \n","30498  @SatchofBridgend @PittMom16 @thehill It is. But we follow laws in this country. Congress can work together to come up with better healthcare. Until mind ur own country thanks                                                          \n","\n","      sentiment   pred  happiness   sadness bi_pred  bi_happiness  bi_sadness  \n","31342  ?         happy  0.898682   0.101318  happy   0.899134      0.100866    \n","32037  ?         happy  0.887219   0.112781  happy   0.886858      0.113142    \n","31983  ?         happy  0.883968   0.116032  happy   0.877798      0.122202    \n","31761  ?         happy  0.873127   0.126873  happy   0.870848      0.129152    \n","30498  ?         happy  0.831058   0.168942  happy   0.842282      0.157718    "]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>pred</th>\n","      <th>happiness</th>\n","      <th>sadness</th>\n","      <th>bi_pred</th>\n","      <th>bi_happiness</th>\n","      <th>bi_sadness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>31057</th>\n","      <td>sad hear jeff work healthcare amp breaks heart desperately need medicare</td>\n","      <td>@JeffHisDudeness So sad to hear this Jeff. I work in healthcare &amp;amp; this just breaks my heart. We desperately need Medicare for ALL.</td>\n","      <td>?</td>\n","      <td>sad</td>\n","      <td>0.092471</td>\n","      <td>0.907529</td>\n","      <td>sad</td>\n","      <td>0.080971</td>\n","      <td>0.919029</td>\n","    </tr>\n","    <tr>\n","      <th>31682</th>\n","      <td>sad thing education healthcare jobs poor sods want jumlaman blogman mercenaries think otherwise</td>\n","      <td>@tavleen_singh And the sad thing is education healthcare jobs is what the poor sods want, but JumlaMan, BlogMan and their mercenaries think otherwise. https://t.co/vls1v4vMLv</td>\n","      <td>?</td>\n","      <td>sad</td>\n","      <td>0.091702</td>\n","      <td>0.908298</td>\n","      <td>sad</td>\n","      <td>0.091563</td>\n","      <td>0.908437</td>\n","    </tr>\n","    <tr>\n","      <th>30307</th>\n","      <td>miserable human must someone want access healthcare ripped away sick people sad</td>\n","      <td>@RickLangel @thehill How miserable of a human being must someone be to want access to healthcare to be ripped away from sick people?  Sad.</td>\n","      <td>?</td>\n","      <td>sad</td>\n","      <td>0.135246</td>\n","      <td>0.864754</td>\n","      <td>sad</td>\n","      <td>0.155372</td>\n","      <td>0.844628</td>\n","    </tr>\n","    <tr>\n","      <th>31075</th>\n","      <td>miss vote healthcare bill</td>\n","      <td>@dccc Did I miss the vote on @RepJayapal's healthcare bill?</td>\n","      <td>?</td>\n","      <td>sad</td>\n","      <td>0.160904</td>\n","      <td>0.839096</td>\n","      <td>sad</td>\n","      <td>0.158897</td>\n","      <td>0.841103</td>\n","    </tr>\n","    <tr>\n","      <th>30509</th>\n","      <td>american healthcare system damn messed neglected it’s sad</td>\n","      <td>the American healthcare system is so damn messed up and neglected it’s sad</td>\n","      <td>?</td>\n","      <td>sad</td>\n","      <td>0.164510</td>\n","      <td>0.835490</td>\n","      <td>sad</td>\n","      <td>0.164923</td>\n","      <td>0.835077</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                            clean_text  \\\n","31057  sad hear jeff work healthcare amp breaks heart desperately need medicare                          \n","31682  sad thing education healthcare jobs poor sods want jumlaman blogman mercenaries think otherwise   \n","30307  miserable human must someone want access healthcare ripped away sick people sad                   \n","31075  miss vote healthcare bill                                                                         \n","30509  american healthcare system damn messed neglected it’s sad                                         \n","\n","                                                                                                                                                                                 text  \\\n","31057  @JeffHisDudeness So sad to hear this Jeff. I work in healthcare &amp; this just breaks my heart. We desperately need Medicare for ALL.                                           \n","31682  @tavleen_singh And the sad thing is education healthcare jobs is what the poor sods want, but JumlaMan, BlogMan and their mercenaries think otherwise. https://t.co/vls1v4vMLv   \n","30307  @RickLangel @thehill How miserable of a human being must someone be to want access to healthcare to be ripped away from sick people?  Sad.                                       \n","31075  @dccc Did I miss the vote on @RepJayapal's healthcare bill?                                                                                                                      \n","30509  the American healthcare system is so damn messed up and neglected it’s sad                                                                                                       \n","\n","      sentiment pred  happiness   sadness bi_pred  bi_happiness  bi_sadness  \n","31057  ?         sad  0.092471   0.907529  sad     0.080971      0.919029    \n","31682  ?         sad  0.091702   0.908298  sad     0.091563      0.908437    \n","30307  ?         sad  0.135246   0.864754  sad     0.155372      0.844628    \n","31075  ?         sad  0.160904   0.839096  sad     0.158897      0.841103    \n","30509  ?         sad  0.164510   0.835490  sad     0.164923      0.835077    "]},"metadata":{"tags":[]}}]},{"metadata":{"id":"xqV8ODiTFhmW","colab_type":"text"},"cell_type":"markdown","source":["While examining the happiest and saddest posts as scored by the unigram and bigram model, I was pleasantly surprised to find that the distinctly negative tweet that had been labeled \"happy\" by the unigram classifier, #31477 (\"@MmeScience @sahilkapur As someone with a pre-existing condition who needs health insurance to survive I'd rather not have my healthcare fucked up so \"the good guys\" can win political points, thanks\") was not included. Although the accuracy was essentially the same for the labeled dataset, there's a chance the performance still improved for something like healthcare, where many double negatives might occur."]},{"metadata":{"id":"iHIihfnBD8BZ","colab_type":"text"},"cell_type":"markdown","source":["### Conclusion"]},{"metadata":{"id":"v74W3d6ND9eh","colab_type":"text"},"cell_type":"markdown","source":["Overall, I would admit that I'm disappointed with my model. With more time or resources, I would consider collecting a much larger dataset, or including some or all stopwords. Still, I believe there is some merit to this model in that it generally makes good predictions for very happy or very sad tweets. Given that I streamed my data over a very limited time period, it's possible that world events, pop culture, or the Twitter community were influencing user behavior in such a way that much of the data was difficult to classify. It might also be interesting to look into a threshold for the \"happy\" or \"sad\" class - for example, if a predicted happy \"score\" is 50.0001, it might be better labeled as neutral rather than happy."]}]}